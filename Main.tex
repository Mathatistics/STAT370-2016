\documentclass[10pt, a4paper]{article}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}

% !Rnw root = Main.Rnw

\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{mathpazo}
\usepackage{setspace}
\usepackage{tabularx}
\usepackage{float}
\usepackage[natbib=true, backend=bibtex, style=numeric, citestyle=authoryear]{biblatex}
\usepackage{pgf, tikz}
\usepackage{listings}
\usepackage{color}
\usepackage{multicol}
\usepackage[font=small,labelfont=sf, font=sf]{caption}
\usepackage{subcaption}

\usetikzlibrary{positioning, shapes, fit, chains}
\addbibresource{references.bib}

% Some Customizations
\onehalfspacing

\title{A compariative study of variable selection methods on diverse nature of data \\ \vspace{0.5cm}
\Large{Compulsory Assignment (STAT 370)}}

\author{Raju Rimal}

\setcounter{secnumdepth}{0}






\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}
\begin{titlepage}
\maketitle
\tableofcontents
\thispagestyle{empty}
\end{titlepage}

\section{Introduction}
Simple and informative models is always preferred to complex model. Different variable selection techniques are compared on the basis of their performance on various nature of data. Table-\ref{tbl:data-n-nature} presents an overview of the datasets used in this study. All the data are pre-processed using different techniques such as baseline correction and smoothing before they are used in this study.

% latex table generated in R 3.2.4 by xtable 1.8-2 package
% Sat May  7 20:04:14 2016
\begin{table}[H]
\centering
\begingroup\small
\begin{tabularx}{0.9\textwidth}{lllX}
  \hline
Datasets & Types & Response Variables & Predictor Variables \\ 
  \hline
\texttt{MALDITOF\_Milk} & Spectrometric & \texttt{cow, goat, ewe} & Mass Spectra (\texttt{MS}) \\ 
  \texttt{NIR\_Raman\_PUFA} & Spectroscopic & \texttt{PUFA\_total, PUFA\_fat} & \texttt{NIR} and \texttt{Raman} \\ 
  \texttt{GeneExpr\_Prostate} & Microarray & \texttt{tumor} & Gene Expression (\texttt{GeneExpr}) \\ 
   \hline
\end{tabularx}
\endgroup
\caption{Overview of Datasets} 
\label{tbl:data-n-nature}
\end{table}


\textbf{MALDI-TOF (Matrix-assisted laser ionization - Time of Flight)} is a mass spectrometric technique known for identifying proteins, peptides and some other ionisable compounds in samples, as explained in \citet{liland2009quantitative} and \citet{} (Fig: \ref{fig:samplePlot}; top-left). 

\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]
\includegraphics[width=\maxwidth]{figure/samplePlot-1} \caption[Sample observations from a) MALDI-TOF (top-left) b) NIR (top-right) c) Raman (bottom-left) and d) GeneExpression (bottom-right)]{Sample observations from a) MALDI-TOF (top-left) b) NIR (top-right) c) Raman (bottom-left) and d) GeneExpression (bottom-right)}\label{fig:samplePlot}
\end{figure}


\end{knitrout}

\textbf{Near Infrared (NIR)} and \textbf{Raman} are spectroscopic techniques where the frequencies of reflected light from a vibrating molecules are measured. In the case of Raman spectroscopy, measurement is made only on raman shift, it is able to detect wider range of molecules than NIR (Fig: \ref{fig:samplePlot}; top-right and bottom-left). Here, the techniques are used to measure the percentage of polyunsaturated fatty acids in a) total sample weight b) total fat content (\cite{naes2013multi}). \textbf{Gene Expression} is a microarray data which contains a Gene Expression Matrix and a response vector which identify whether the sample contains tumor or not. A raw microarray data are images which are transformed into gene expression matrix that contains gene for each sample (fig-\ref{fig:samplePlot} (bottom-right)).





% !Rnw root = Main.Rnw

\section{Statistical Models}

\subsection{Principal Component Analysis (PCA)}
PCA enables the underlying structure present in a dataset by transforming variables into a new set of uncorrelated variables. PCA is used to reduce the dimension of dataset consisting of correlated variables, retaining most of the variation present in it on first few PCs.
  
In this study, the first and second principal components are plotted, in figure - \ref{fig:scorePlots} (top-left), which are linear combinations of original variables. The PC of MALDI-TOF are colored according to milk proportion of cow, goat and ewe which revels clusters present in the mass-spectra at three different corners. A similar grouping is visible on gene expression data in fig-\ref{fig:scorePlots} (top-right) where the points are colored according to there state of having tumor or not.

Further, the principal components of NIR and Raman datasets in figure - \ref{fig:scorePlots} (bottom) shows that higher concentration of PUFA are separated by first principal component in NIR and second principal component in Raman dataset.





\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]
\includegraphics[width=\maxwidth]{figure/scorePlots-1} \caption[Principal components of (top-left) MALDI-TOF dataset with colored proportion of cow, goat and ewe milk, (top-right) Gene Expression dataset with presence and absense of tumer on the sample, (bottom) NIR and Raman dataset with opacity factor with percentage of (left) PUFA in total sample and (right) PUFA in total fat content]{Principal components of (top-left) MALDI-TOF dataset with colored proportion of cow, goat and ewe milk, (top-right) Gene Expression dataset with presence and absense of tumer on the sample, (bottom) NIR and Raman dataset with opacity factor with percentage of (left) PUFA in total sample and (right) PUFA in total fat content}\label{fig:scorePlots}
\end{figure}


\end{knitrout}

\subsection{Calibration and Validation}
Model validation ensures how the model will perform on completely new environment, i.e. with observations on included for its calibration. For \textit{external validity}\footnote{\cite[ch.10]{martens2001multivariate}}, datasets are splitted into training and test sets with 70 percent of observations taken as training set while rest as test set. However, due to replications, MALDI-TOF milk data is splitted into 3:2 ratio such that the replications are bind together. An \textit{internal validation}\footnote{\cite[ch.10]{martens2001multivariate}} is done on the training set with 10 fold random cross-validation except on MALDI-TOF milk. On MALDI-TOF data a consecutive splitting is made with 30 folds which hence ensure each set of replication creates a fold. Further, the models calibrated with cross-validation from training set are used for predicting test set.

\subsection{Partial Least Square (PLS) Regression}
PLS uses the latent structure for modeling the relation between matrices $\mathbf{X}$ and $\mathbf{Y}$. Latent variables of both $\mathbf{X}$ and $\mathbf{Y}$ are modeled to explain the variance structure (direction) present in $\mathbf{Y}$. Flawless performance on wide matrices (where variables are more than observations) and correction of multicollinearity are the advantages of Partial Least Square Regression.








\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]
\includegraphics[width=\maxwidth]{figure/ValidationPlots-1} \caption[Maximum variation explained on response in Milk, NIR and Raman dataset and maximium correct classification proportion for gene Expression dataset]{Maximum variation explained on response in Milk, NIR and Raman dataset and maximium correct classification proportion for gene Expression dataset. Each line represent a model for which the R2 predicted is calculated with number of components chosen by cross-validation (end of each line).}\label{fig:ValidationPlots}
\end{figure}


\end{knitrout}

A PLS model with cross-validation is fitted on training set of all the datasets with individual responses (i.e. one response per model). A measure of maximum $R^2$ predicted is used as a validation tool. The number of components, needed for explaining most of the variation present in the response, obtained from cross-validation is used for test prediction. For classification of tumor or non-tumor in Gene Expression dataset, a modified PLS model is used where the scores from PLS model, with number of components needed for minimum RMSECV, is used as predictor variable in Linear Discriminant Analysis (LDA) model. A final model, with maximum average correct classification proportion, is used to predict the test set.

Mostly, the variation explained in test are smaller than cross-validation in fig - \ref{fig:ValidationPlots} while on few of them it is larger than cross-validation. Plots from spectroscopic data shows that NIR data are able to explain \textit{PUFA present in total} better than Raman while Raman is better is explaining \textit{PUFA in fat}. Since number of components from cross-validation are used, the model fitted for goat response in MALDI-TOF data has 70 components and is performing poor in test data for which the presence of noise with inclusion of more components may be the reason behind.  However, in all the cases, more than 65\% of the variation on test is explained by the chosen PLS model.


% !Rnw root = Main.Rnw
\subsection{Linear Discriminent Analysis}
Assuming equal variance for each group, LDA attempts to classify/ discriminate the response $\mathbf{y}$ using a log-odd function, linear on $\mathbf{X}$, i.e, LDA decision boundaries are linear (\cite{friedman2001elements}). Subjects with and without tumor, in Gene Expression data are classified using a LDA model from PLS scores. The number of components is obtained from a cross-validation technique, as in figure - \ref{fig:classifyPlot}, which integrates LDA and PCA to obtain optimized number of components that minimize the misclassification error. The first Linear Discriminant score plotted for training and testset for their prediction (fig - \ref{classifyPlot} (right)) shows that the model has classified the presence of tumor with high accuracy.





\begin{figure}[H]
\centering
\begin{subfigure}[t]{0.7\textwidth}
    \resizebox{\linewidth}{2.8in}{
    \begin{tikzpicture}[
    rectNode/.append style = {draw, rectangle, minimum height = 1cm, minimum width = 1cm},
    myArrow/.append style = {->, >=latex, dashed},
    node distance = 0.8cm,
    scale = 0.7
    ]
      \node[rectNode, label = {[name = cvTestLabel]cvTest}] (cvtest) {$j$};
      \node[rectNode, right = of cvtest, label = cvTrain] (cvtrain) {$1,\ldots j-1, j+1, \ldots, 10$};
      
      \node[rectNode, below = of cvtrain](models) {Partial Least Squares Model};
      \node[rectNode, right = of models, label = Scores](scores) {1, 2, \ldots, };
      
      \node[rectNode, below = of models.195, text width = 5cm, rectangle split, rectangle split parts=2, label = {[text width = 5cm, align = center, name = ldaDataLabel]below:{\small Loop over each additional scores}}](ldaData) {
      $y =$ \texttt{tumor}, \\$X = $ \texttt{scores} from $1$ to $i$
      	\nodepart{second}
      	LDA Model: $y = f(X)$
      	};
      
      \node[text width = 3.5cm, align = center, rectNode, below = of scores, label = {[text width = 3.5cm, align = center, name = errorLabel]below:{\footnotesize For $i^{th}$ Scores \vspace{-3pt} \\ \footnotesize and $j^{th}$ cv split}}](error){Misclassificaton Error\\(cvTest and Train)};
      
      \node[fit=(cvTestLabel)(errorLabel)(error)(ldaData)(ldaDataLabel)(scores), draw, label = below:{Cross-validation Loop on Training set of Gene Expression Data}] {};
      
      %% Arrows
      
      \draw[myArrow] (cvtest.south) -- (cvtest |- ldaData.north);
      \draw[myArrow] (cvtrain) -- (models);
      \draw[myArrow] (models) -- (scores);
      \draw[myArrow] (scores.south) -- ++(0pt, -3mm)  -| (ldaData.north);
      \draw[myArrow] (ldaData.east) |- (error);
    \end{tikzpicture}
    }
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.25\textwidth}
    \includegraphics[height = 2.8in]{figure/classifPlot}
\end{subfigure}
\caption{(left) Cross Validation with LDA integrated within PLS model for classification problem in Gene Expression Data (right) Test and Training classification}
\label{fig:cvloop}
\end{figure}


% !Rnw root = Main.Rnw
\section{Variable Selection}

Variable selection techniques as suggested in \citet{mehmood2012review} are implemented on each dataset and a comparison is made with the model fitted with complete set. \citet{mehmood2012review} have categorized the variable selection technique into three different categories: Filter, Wrapper and Embedded methods (fig. - \ref{fig:varSelMethods}). 

\begin{figure}[H]
\begin{center}
    \includegraphics[width=\textwidth]{figure/varSelMethods.png}
\end{center}
\caption{Illustration of Variation Selection Techniques}
\label{fig:varSelMethods}
\end{figure}

\subsection{Filter Method}
Filter method selects variables from a fitted PLS model introducing a threshold on some measure such as Loading Weights, Regression Coefficients and Variable Importance in Projection (VIP). VIP is used as a filter method in this study with a threshold value of 1. Since, only the important variables are selected and with reduced noise, in many occasions, PLS model fitted with only selected variables predicts better than the full Model. For instance, with cow milk as response in the case of MALDI-TOF model, figure - \ref{fig:VIPcowCompare} shows that the model from VIP filter with only 1305 variables, has explained more than the Full Model on both CV and testset.

\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]
\includegraphics[width=\maxwidth]{figure/VIPcowCompare-1} \caption[Comparison of Models with complete set of variables and variables obtained from VIP filter]{Comparison of Models with complete set of variables and variables obtained from VIP filter}\label{fig:VIPcowCompare}
\end{figure}


\end{knitrout}

\subsection{Wrapper Method}
Wrapper Method uses filter method in an iterative way. MCUVE (Monte-Carlo based Uninformative Variable Elimination) and Shaving methods are used in this study. The comparison 

\textbf{MCUVE} method is used which splits the sample set into test and train and runs random cross-validation on training and selects variables based on final performance test on test data. Despite decrease the risk of over-fitting (\cite{mehmood2012review}), MCUVE PLS adopted here has chosen very few variables resulting low R-Sq predicted in all nature of datasets.

\textbf{Shaving} method (\cite{shavingCRAN}) first arrange the variables using some filter methods. A subset of least information variables are eliminated using a threshold value and a model is again fitted and model performance is measured. The procedure is repeated to achieve maximum model performance.

\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]
\includegraphics[width=\maxwidth]{figure/ShavingMCUV-1} \caption[(left) Variables selected on MALDI-TOF Milk Spectra data where goat is a response]{(left) Variables selected on MALDI-TOF Milk Spectra data where goat is a response. (right) Maximum $R^2$ Predicted from model with variables selected from Fullmodel, MCUV, Shaving method.}\label{fig:ShavingMCUV}
\end{figure}


\end{knitrout}

MCUV method is able to explain only 20\% of variation present in goat percent in the milk mixture of MALDI-TOF data (fig - \ref{fig:ShavingMCUV} (right)). Selection of few variables by the method has removed many more information which results in this poor performance. While the shaving method with less than 5000 variables has almost performed similarly as complete model with more than 6000 variables. The comparison between these methods will be performed on next section of this paper.

\subsection{Embedded Method}

Embedded Method integrate variable selection procedure in a single run of model fitting. Since the procedure is wrapped within PLS algorithm, it is less time consuming than wrapper method where double iteration occurs. Truncation, an embedded method, is used in this study where, loading weights are truncated around their median based on their confidence intervals. For Instance, in the case of MALDI-TOF milk data fitted for response \texttt{cow} and Gene Expression data (Figure - \ref{fig:truncPlot}), the loading weights closer to zero with 95\% confidence level around median are termed as uninformative and set to zero or truncated. For each additional components, the truncation method truncates more loading weights since the higher order components contains extra noise.

\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]
\includegraphics[width=\maxwidth]{figure/truncPlot-1} \caption[Histogram of Loading Weights for full and truncated model for (left) MALDI-TOF milk data with response cow (right) Gene Expression data]{Histogram of Loading Weights for full and truncated model for (left) MALDI-TOF milk data with response cow (right) Gene Expression data. Corresponding inset shows the maximum R-sq predicted for train, CV and test.}\label{fig:truncPlot}
\end{figure}


\end{knitrout}


% !Rnw root = Main.Rnw

\section{Analysis and Discussion}



Small degree of freedom arises difficulties in studying the interaction between dataset and variable selection methods implemented on them. So, replications of complete study are created specifying different test and training sets. Since, 30 percent of observations are sampled randomly as a test set in NIR, Raman and Gene datasets, replications 2 and 3 has also implemented the same strategy. However, in the case of MALDI-TOF milk dataset, with constrained to keep replicates together, the second and third replicates are created by keeping 120 observations as training set starting from position 41 and 61 respectively. $R^2$ predicted for test observation from all replicated are used for further analysis (Figure - \ref{fig:testPlot}).


\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]
\includegraphics[width=\maxwidth]{figure/testPlot-1} \caption[Boxplot for each data and method combination subdivided into their respective responses]{Boxplot for each data and method combination subdivided into their respective responses. Model from MCUVE in NIR and Raman data have drastically poor performance than other.}\label{fig:testPlot}
\end{figure}


\end{knitrout}

Since each responses are confined to their respective datasets and we are specifically interested on the methods, a nested mixed effect model is adopted in this study as eq-\ref{eq:modelEq} where methods are kept fixed and the data and response nested on them are considered random, i.e each data and response nested on their respective dataset have different intercept. The model is written as,

\begin{equation}
\label{eq:modelEq}
  y_{ijkl} = \mu + \alpha_{i} + \beta_{j} + (\alpha\beta)_{ij} + \gamma_{k(j)} + \epsilon_{ik(j)l}
\end{equation}
Where, $\epsilon \sim N(0, \sigma^2)$ and the random and fixed effects are assumed to satisfy following assumptions:
\begin{align*}
\sum_{i = 1}^5 {\alpha_i} &= 0 && \beta_{j} \sim N(0, \sigma_\beta^2) &&
\gamma_{k(j)} \sim N(0, \sigma_{\gamma(\beta)}^2)
\end{align*}
Here, $i = 1, \ldots 5$ (Methods), $j = 1, \ldots, 4$ (Data), $k = 1, \ldots, n(j)$ (Response nested under data) and $l = 1, 2, 3$ (Replication)






\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]
\includegraphics[width=\maxwidth]{figure/rvfPlot-1} \caption[Residuals vs Fitted Plot]{Residuals vs Fitted Plot}\label{fig:rvfPlot}
\end{figure}


\end{knitrout}

On fitting the model in eq-\ref{eq:modelEq}, due to non homogeneous residuals and negative values in the response variable $R^2$ predicted, we transformed the response as, $\log\left[(p * (1 - p)) + 1\right]$. The transformed model shows that methods are significant while data are not however the residual vs fitted plot suggested three (20, 21 and 101) models all from using MCUVE method on NIR data as outlier. However, the significance of the factors did not change even without these outlier. The residual vs fitted plot from the second model suggested 3 models (63, 19 and 103) which still are from using MCUVE method. Finally, a model without MCUVE method is fitted which has suggested that there is not any significance difference between different data and different methods but there is significant interaction between them.


\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]
\includegraphics[width=\maxwidth]{figure/anovaPlot-1} \caption[Plot from Anova table]{Plot from Anova table. Number on top of each bars are p-value}\label{fig:anovaPlot}
\end{figure}


\end{knitrout}
\section{Conclusion}
Results from ANOVA, plotted in figure - \ref{fig:anovaPlot}, was expected since the MCUVE method of variables selection has selected few variables and has lost considerable amount of information. However, Model 3, without MCUVE method, shows that the variables selection methods performs differently on different nature of dataset (significant interaction between method and data). Furthermore, in all three models, the response which is nested under data are significant, i.e. responses for given data are significantly different.

On this comparison of variable selection methods, we found that there is no significance difference in variable selection methods apart from MCUVE which is rather conservative in selecting variables. However, the methods can perform differently according the nature of data. So, it is desirable to test the performance of variable selection before implementing them on any analysis.

\appendix

% !Rnw root = Main.Rnw

\section{Codes in Use}
\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mymauve}{rgb}{0.58,0,0.82}
\lstinputlisting[
backgroundcolor=\color{white},
breakatwhitespace=false,
keepspaces=false,
morekeywords={*,...},
numbers=none,
showspaces=false,
showstringspaces=false,
showtabs=false,
stepnumber=2,
stringstyle=\color{mymauve},
% title=\Large{\textsc{Code used in this thesis}},
numbersep=5pt,
numberstyle=\tiny\color{mygray}, 
breaklines=true,
language=R,
basicstyle=\ttfamily\footnotesize,
columns=fullflexible,
commentstyle=\color{mygreen},
keepspaces=true,
keywordstyle=\color{blue},
tabsize=2,
]
{CodeFile.R}


\printbibliography
\end{document}
